{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align='center'>Final Analysis</h1>\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, we faced issues with lack of uniformity among different data sources. E.g. we could not effectively combine the data from imdb and Wikipedia together. Although we had planned to use movie review texts from Rottentomatoes, their API approval came quite late and we had to drop that idea. We believe pooling text data from all different sources possible would yield much better and more accurate results for the type of analysis we performed. That will also mean that we will then need to use a cluster computing infrastructure. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Modeling\n",
    "\n",
    "We first tried out LSI and discarded this angle due to the poor quality of the results. We then applied topic modeling with LDA on movie plots and this yielded interesting and plausible results. We discovered the primary themes that are present in films and could identify differences in the films from different cultures. We also saw that the movie preferences of moviegoers from 6 different countries are very similar for the years since 2007. It would have been interesting to see if this also applied in earlier times, but it was not possible for us to acquire such data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distributed Word Representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We selected a few 1000 relatively longer plot summaries from Wikipedia for applying the doc2vec algorithm on. We saw that using very short plots add a lot of noise. Also, for doc2vec instead of using tagged sentences, we used the entire plot text as one tagged document and that significantly improved the results. For individual movies, the similarities are not quite strong, but when we look at larger collections like all movies from different countries, the algorithm is able to learn the regularities very effectively. This indicates if we have longer text descriptions for each movie, we will get better results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
